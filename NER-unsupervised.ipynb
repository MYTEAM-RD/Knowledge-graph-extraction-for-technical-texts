{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e66d832b",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5822441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spacy\n",
    "import itertools\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "from transformers import pipeline\n",
    "from itertools import groupby, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea611faa",
   "metadata": {},
   "source": [
    "# Camembert vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdce1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gilf/french-camembert-postag-model\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"gilf/french-camembert-postag-model\")\n",
    "texts = \"L’innovation principale réside dans l’aspect transverse de l’échange de données qu’elle permet, et ce sans données perte de qualité ni d’intégrité, entre tous les acteurs du métier du SLI, de l’analyse de la maintenance, en passant par la documentation technique, la gestion des approvisionnements et ainsi le maintien en condition opérationnelle du système tout au long de sa durée de vie.\"\n",
    "tokens = tokenizer.tokenize(texts)\n",
    "\n",
    "#Extract the POS of tokens\n",
    "nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "list_tokens = nlp_token_class(texts)\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "#Get rid of '_' and \"''\" in the beginning of the tokens\n",
    "\n",
    "for i,item in enumerate(list_tokens):\n",
    "    word_1 = item.get('word')\n",
    "    if word_1[0] == \"’\" and len(word_1)>1:\n",
    "        item['word'] =  word_1[1:]\n",
    "        \n",
    "for i,item in enumerate(tokens):\n",
    "    if item[0] == '▁':\n",
    "        tokens[i] = item[1:]\n",
    "\n",
    "        \n",
    "#Create a list of indexed tokens\n",
    "final_list = []\n",
    "for i, item in enumerate(tokens):\n",
    "    for item_2 in list_tokens:        \n",
    "        if item_2.get('word')== item:\n",
    "             final_list.append((i, item_2.get('entity_group'), item))      \n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "#Create a list of nouns\n",
    "noun_list=[]\n",
    "for item in set(final_list):\n",
    "    if item[1] == 'NC' or item[1] == 'NPP': \n",
    "        noun_list.append(item)\n",
    "\n",
    "# Remove stop words         \n",
    "filterd_noun_list=[]\n",
    "for item in noun_list:\n",
    "    if item[2].isalpha():\n",
    "        filterd_noun_list.append(item)\n",
    "\n",
    "#A list only of names without indexes and  POSs       \n",
    "just_nouns=[]\n",
    "for item in filterd_noun_list:\n",
    "    just_nouns.append(item[2])\n",
    "    \n",
    "    \n",
    "#Create a list of repeated nouns    \n",
    "noun_counter = Counter(just_nouns)  \n",
    "repeated_noun = []\n",
    "for item in filterd_noun_list:\n",
    "    if noun_counter[list(item)[2]]>1:\n",
    "        repeated_noun.append(item)\n",
    "sorted_repeated_noun= sorted(repeated_noun, key=lambda tup: tup[2], reverse=True)\n",
    "\n",
    "\n",
    "#Create a dictionary of nouns so that Key: nouns, key.value: index of noun\n",
    "noun_dictionary = {}\n",
    "for idx_token,dep_tok,tok in filterd_noun_list:\n",
    "        noun_dictionary.setdefault(tok, []).append((idx_token))\n",
    "        \n",
    "        \n",
    "#Extract the last layer of Camembert vector\n",
    "model = CamembertModel.from_pretrained('camembert-base')\n",
    "input_ids = torch.tensor(tokenizer.encode(texts, add_special_tokens=True)).unsqueeze(0)\n",
    "output = model(input_ids)\n",
    "final_output_camembert_list = output.last_hidden_state\n",
    "\n",
    "\n",
    "\n",
    "final_list_=[]\n",
    "for key in noun_dictionary:\n",
    "    new_list=[]\n",
    "    for item in noun_dictionary.get(key):\n",
    "        new_list.append(final_output_camembert_list[0][item+1])\n",
    "    final_list_.append(new_list)\n",
    "        \n",
    "#Convert to np array       \n",
    "final_list_np = []\n",
    "for item in final_list_:\n",
    "    np_list=[]\n",
    "    for item_1 in item:\n",
    "        np_list.append(item_1.detach().numpy())\n",
    "    final_list_np.append(np_list)\n",
    "        \n",
    "#mean velue of repeated nouns\n",
    "final_list_mean = []\n",
    "for item in final_list_np:\n",
    "    final_list_mean.append(np.mean(item, axis=0))\n",
    "    \n",
    "    \n",
    "# Calculate Cosine Similarity\n",
    "cos_sim_final = []\n",
    "for item in list(combinations(final_list_mean, 2)):\n",
    "    outputs_1 = item[0]\n",
    "    outputs_2 = item[1]\n",
    "    cos_sim_final.append(np.dot(outputs_1, outputs_2)/(np.linalg.norm(outputs_1)*np.linalg.norm(outputs_2)))\n",
    "    \n",
    "#Remove tokens that do not exist in the Lexique383\n",
    "lex = pd.read_csv('http://www.lexique.org/databases/Lexique383/Lexique383.tsv', sep='\\t')\n",
    "df1 = lex[lex['cgram'] == 'NOM']\n",
    "list_lex = df1['ortho'].to_list()\n",
    "# Keep only the names in the Lex dictionary \n",
    "flitered_dict_noun = [x for x in list(noun_dictionary.keys()) if x in list_lex]\n",
    "\n",
    "# Delete all incorrectly split tokens and recalculate all similarities \n",
    "lex = pd.read_csv('http://www.lexique.org/databases/Lexique383/Lexique383.tsv', sep='\\t')\n",
    "df1 = lex[lex['cgram'] == 'NOM']\n",
    "list_lex = df1['ortho'].to_list()\n",
    "# Keep only the names in the Lex dictionary \n",
    "flitered_dict_noun = [x for x in list(noun_dictionary.keys()) if x in list_lex]\n",
    "\n",
    "\n",
    "flitered_dict = {}\n",
    "for idx_token,dep_tok,tok in filterd_noun_list:\n",
    "    if tok in flitered_dict_noun:\n",
    "        flitered_dict.setdefault(tok, []).append(idx_token)\n",
    "        \n",
    "flitered_final_list_=[]\n",
    "for key in flitered_dict:\n",
    "    new_list=[]\n",
    "    for item in flitered_dict.get(key):\n",
    "        new_list.append(final_output_camembert_list[0][item+1])\n",
    "    flitered_final_list_.append(new_list)\n",
    "    \n",
    "    \n",
    "filtered_final_list_np = []\n",
    "for item in flitered_final_list_:\n",
    "    np_list=[]\n",
    "    for item_1 in item:\n",
    "        np_list.append(item_1.detach().numpy())\n",
    "    filtered_final_list_np.append(np_list)\n",
    "    \n",
    "filtered_final_list_mean = []\n",
    "for item in filtered_final_list_np:\n",
    "    filtered_final_list_mean.append(np.mean(item, axis=0))\n",
    "\n",
    "filtered_cos_sim_final = []\n",
    "for item in list(combinations(filtered_final_list_mean, 2)):\n",
    "    outputs_1 = item[0]\n",
    "    outputs_2 = item[1]\n",
    "    filtered_cos_sim_final.append(np.dot(outputs_1, outputs_2)/(np.linalg.norm(outputs_1)*np.linalg.norm(outputs_2)))\n",
    "    \n",
    "    \n",
    "filtered_list_0_50 = []\n",
    "filtered_list_50_60 = []\n",
    "filtered_list_60_70 = []\n",
    "filtered_list_70_80 = []\n",
    "filtered_list_80_90 = []\n",
    "filtered_list_90_100 = []\n",
    "filtered_comb_noun_dictionary = list(combinations(list(flitered_dict.keys()), 2))\n",
    "for indx, item in enumerate(filtered_cos_sim_final):\n",
    "    if 0<item<0.5:\n",
    "        filtered_list_0_50.append((filtered_comb_noun_dictionary[indx],item))\n",
    "    elif 0.5<item<0.6:\n",
    "        filtered_list_50_60.append((filtered_comb_noun_dictionary[indx],item))\n",
    "    elif 0.6<item<0.7:\n",
    "        filtered_list_60_70.append((filtered_comb_noun_dictionary[indx],item))\n",
    "    elif 0.7<item<0.8:\n",
    "        filtered_list_70_80.append((filtered_comb_noun_dictionary[indx],item))\n",
    "    elif 0.8<item<0.9:\n",
    "        filtered_list_80_90.append((filtered_comb_noun_dictionary[indx],item))\n",
    "    elif item>0.9:\n",
    "        filtered_list_90_100.append((filtered_comb_noun_dictionary[indx],item))\n",
    "        \n",
    "ls_cos_sim_all_filtered = filtered_list_0_50+filtered_list_50_60+filtered_list_60_70+filtered_list_70_80+filtered_list_80_90+filtered_list_90_100\n",
    "\n",
    "ls_cos_sim_all_filtered = filtered_list_0_50+filtered_list_50_60+filtered_list_60_70+filtered_list_70_80+filtered_list_80_90+filtered_list_90_100\n",
    "ls_cos_sim_all_filtered_final_0 = []\n",
    "for item in ls_cos_sim_all_filtered:\n",
    "    ls_cos_sim_all_filtered_final_0.append((item[0][0],item[0][1],item[1]))\n",
    "\n",
    "ls_cos_sim_all_filtered_final_1=sorted(ls_cos_sim_all_filtered_final_0, key=lambda tup: (tup[0],tup[2]) , reverse=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2a203",
   "metadata": {},
   "source": [
    "# Since our data is private, we are not allowed to publish it and only one sentence is used in this file as an example. The code to get all the variables used in the knowledge graph file is in the next block, but since the input sentence is too small, you will get an error. You can get these values using your own text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44220f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the words with cosine similarity > 0.7\n",
    "ls_cos_sim_all_filtered_final_2 = {}\n",
    "for w_1,w_2,cs in ls_cos_sim_all_filtered_final_1:\n",
    "    if cs > 0.70:\n",
    "        ls_cos_sim_all_filtered_final_2.setdefault(w_1, []).append((w_2,cs))\n",
    "#################################################################################        \n",
    "#elete lists that are less than 10 in length\n",
    "ls_cos_sim_all_filtered_final_3={}\n",
    "for item in ls_cos_sim_all_filtered_final_2.keys():\n",
    "    if len(ls_cos_sim_all_filtered_final_2[item])>=10:\n",
    "          ls_cos_sim_all_filtered_final_3.setdefault(item, []).append(ls_cos_sim_all_filtered_final_2[item]) \n",
    "#################################################################################\n",
    "\n",
    "best_words = []\n",
    "for item in ls_cos_sim_all_filtered_final_3.keys():\n",
    "    first_ls= [item]\n",
    "    for item_2 in range(len(ls_cos_sim_all_filtered_final_3[item][0])):\n",
    "        first_ls.append(ls_cos_sim_all_filtered_final_3[item][0][item_2][0])\n",
    "    best_words.append(first_ls)\n",
    "#################################################################################\n",
    "len_list= []\n",
    "for item in best_words:\n",
    "    len_list.append(len(item))\n",
    "    \n",
    "avg_l = np.mean(len_list)\n",
    "max_l = np.max(len_list)\n",
    "min_l = np.min(len_list)\n",
    "print(f'mean is {avg_l}, max is {max_l}, min is {min_l}')\n",
    "#################################################################################\n",
    "best_words.sort(key=len, reverse=True)\n",
    "com_best_words = list(combinations(best_words, 2))\n",
    "\n",
    "\n",
    "list_common_words_sorted = []\n",
    "for item_1,item_2 in  enumerate(com_best_words):\n",
    "    list3 = set(item_2[0])&set(item_2[1])\n",
    "    list4 = sorted(list3, key = lambda k : item_2[0].index(k))\n",
    "    list_common_words_sorted.append(list4)\n",
    "    \n",
    "    \n",
    "list_to_drop = []\n",
    "for idx,item in enumerate(com_best_words):\n",
    "    l_0 = len(list_common_words_sorted[idx])\n",
    "    l_1 = len(item[0])\n",
    "    l_2 = len(item[1])\n",
    "    if l_0>(l_2*0.40):\n",
    "        list_to_drop.append(item[1])\n",
    "        \n",
    "# Creat the final list of 50 sub-lists         \n",
    "final_list_50=[]\n",
    "for element in best_words:\n",
    "    if element not in list_to_drop:\n",
    "        final_list_50.append(element)\n",
    "\n",
    "#Associate the cammembert vector of each word in list \n",
    "Noun_1 = list(flitered_dict.keys())\n",
    "cam_vec_final_list_50 = []\n",
    "for item in final_list_50:\n",
    "    temp_list = []\n",
    "    for item_1 in item:\n",
    "        idx = Noun_1.index(item_1)\n",
    "        temp_list.append(filtered_final_list_mean[idx])\n",
    "    cam_vec_final_list_50.append(temp_list)\n",
    "    \n",
    "#Associate the label of each cammembert vector according to the number of their sublist    \n",
    "total_cam_vec_final_list_50 = []\n",
    "for item in total_cam_vec_final_list_50:\n",
    "    temp_ls = []\n",
    "    for item_1 in item:\n",
    "        temp_ls.append(item_1)\n",
    "    total_cam_vec_final_list_50.append(temp_ls)\n",
    "    \n",
    "labeled_cam_vector_data=[]\n",
    "for idx, item in enumerate(total_cam_vec_final_list_50):\n",
    "    for item_1 in item:\n",
    "        labeled_cam_vector_data.append((list(item_1),idx))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45416022",
   "metadata": {},
   "source": [
    "# Train a LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f69b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(labeled_cam_vector_data), 768))\n",
    "y = []\n",
    "for idx,item in enumerate(labeled_cam_vector_data):\n",
    "    y.append(item[1])\n",
    "    for i in range(len(item[0])):\n",
    "        X[idx,i] = item[0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8120e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr_2 = LogisticRegression()\n",
    "logisticRegr_2.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
